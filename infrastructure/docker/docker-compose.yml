version: "3.8"

# =============================================================================
# E-Commerce Analytics Pipeline — Local Infrastructure
# Services: Zookeeper, Kafka, Kafka-UI, Spark, MinIO (local S3), Airflow
# =============================================================================

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
    # AWS/MinIO credentials for Airflow to talk to our local S3
    AWS_ACCESS_KEY_ID: minioadmin
    AWS_SECRET_ACCESS_KEY: minioadmin
    AWS_DEFAULT_REGION: us-east-1
  volumes:
    - ../../airflow/dags:/opt/airflow/dags
    - ../../airflow/logs:/opt/airflow/logs
    - ../../airflow/plugins:/opt/airflow/plugins
  depends_on:
    postgres:
      condition: service_healthy

services:
  # ---------------------------------------------------------------------------
  # 1. ZOOKEEPER — Kafka's coordination service
  #    Kafka brokers register with Zookeeper; it tracks leader election.
  # ---------------------------------------------------------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # 2. KAFKA BROKER — The message bus
  #    Producers write events; consumers read them. Topics partition data.
  # ---------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"       # External access (your local Python scripts)
      - "29092:29092"     # Internal Docker network access
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 24
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10

  # ---------------------------------------------------------------------------
  # 3. KAFKA-UI — Visual dashboard for Kafka topics/messages
  #    Access at http://localhost:8080
  # ---------------------------------------------------------------------------
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

  # ---------------------------------------------------------------------------
  # 4. KAFKA TOPIC INITIALIZER — Creates our topics on startup
  # ---------------------------------------------------------------------------
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      echo 'Creating Kafka topics...'
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic ecommerce.user_clicks
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic ecommerce.orders
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic ecommerce.inventory_cdc
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic ecommerce.processed_events
      echo 'Topics created!'
      kafka-topics --list --bootstrap-server kafka:29092
      "

  # ---------------------------------------------------------------------------
  # 5. SPARK MASTER — Coordinates distributed batch/streaming jobs
  #    Access Spark UI at http://localhost:8081
  # ---------------------------------------------------------------------------
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    ports:
      - "8081:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master port (workers connect here)
    volumes:
      - ../../spark:/opt/spark-apps
      - spark-data:/opt/spark-data

  # ---------------------------------------------------------------------------
  # 6. SPARK WORKER — Executes tasks assigned by the master
  # ---------------------------------------------------------------------------
  spark-worker:
    image: bitnami/spark:3.5.0
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    depends_on:
      - spark-master
    volumes:
      - ../../spark:/opt/spark-apps
      - spark-data:/opt/spark-data

  # ---------------------------------------------------------------------------
  # 7. MINIO — Local S3-compatible object storage (our "Data Lake")
  #    Access at http://localhost:9001  (user: minioadmin / minioadmin)
  #    This replaces real AWS S3 for local development.
  # ---------------------------------------------------------------------------
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"   # S3 API endpoint
      - "9001:9001"   # MinIO Web Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # 8. MINIO BUCKET INITIALIZER — Creates our S3 buckets on startup
  # ---------------------------------------------------------------------------
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      mc alias set local http://minio:9000 minioadmin minioadmin
      mc mb --ignore-existing local/ecommerce-raw-events
      mc mb --ignore-existing local/ecommerce-processed
      mc mb --ignore-existing local/ecommerce-curated
      mc mb --ignore-existing local/spark-checkpoints
      echo 'Buckets created!'
      mc ls local
      "

  # ---------------------------------------------------------------------------
  # 9. POSTGRES — Backend database for Airflow metadata
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5

  # ---------------------------------------------------------------------------
  # 10. AIRFLOW WEBSERVER — DAG UI at http://localhost:8082
  #     (user: airflow / airflow)
  # ---------------------------------------------------------------------------
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ---------------------------------------------------------------------------
  # 11. AIRFLOW SCHEDULER — Triggers DAG runs on schedule
  # ---------------------------------------------------------------------------
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler

  # ---------------------------------------------------------------------------
  # 12. AIRFLOW DB INIT — Runs once to set up Airflow's metadata tables
  # ---------------------------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com

volumes:
  minio-data:
  postgres-data:
  spark-data:
